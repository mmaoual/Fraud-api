# -*- coding: utf-8 -*-
"""Projet : FRAUD

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rem5O7Ynb3x5ua9pC2w_RY_4l6rBrAEE

# **PROJET : FRAUD**

**Sommaire :**   
- Préparation du jeu de données
- Exploration et Statistiques descriptives
- Data processing
- Visualisation (Matplotlib)
- Machine learning
- Conclusion



---

**Objectif :** *Prédire si une transaction est frauduleuse*

## **I-** Préparation du jeu de données :

Audit et exploration des données

### Chargement du jeu de données et desciption

> Description du dataframe et des variables
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score , cross_validate , StratifiedKFold,KFold
from sklearn.linear_model import LogisticRegression,LinearRegression
from sklearn.metrics import confusion_matrix,accuracy_score, balanced_accuracy_score,mean_squared_error, mean_absolute_error, r2_score
from imblearn.over_sampling import SMOTE
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler

# Importer les données du fichier Fraud.csv dans un DataFrame
dfTransaction = pd.read_csv('https://assets-datascientest.s3-eu-west-1.amazonaws.com/de/total/fraud.csv')

# Afficher un aperçu des transactions (10 premières lignes)
dfTransaction.head(10)

# Affichage des dimensions du DataFrame dfTransaction
print(dfTransaction.shape)

# Description du dataframe
dfTransaction.describe()

dfTransaction.info()

"""### Data cleaning : Nettoyage des Données et Gestion des NAs :

> Repérer et supprimer les doublons.
>
> Modifier les éléments et leur type (construction de telle variable, replace, rename et astype).
>
> Détecter les NA (nombre de valeurs manquantes, isna suivie des méthodes any et sum).
>
> Les remplacer (stratégie de remplacement : fillna et des méthodes statistiques).
>
> Les supprimer grâce à la méthode dropna.
>
> Recréer des variables temporelles

"""

# Affichage de la somme des doublons >>> Pour calculer la somme des booléens, on considère que True vaut 1 et False vaut 0.
print(dfTransaction.duplicated().sum())

# On détecte les COLONNES contenant au moins une valeur manquante
dfTransaction.isna().any(axis = 0).sum()

# Aucune valeures manquantes dans les COLONNES

# On détecte les LIGNES contenant au moins une valeur manquante
dfTransaction.isna().any(axis = 1).sum()

# Aucune valeures manquantes dans les LIGNES

"""**-> On constate qu'il n'y a aucune valeur manquante dans le dataFrame**

"""

dfTransaction.dtypes

#On affiche les differents types

# Création de la variable time_diff = différence entre l'heure de la connextion et l'heure d'achat
dfTransaction['signup_time'] = pd.to_datetime(dfTransaction['signup_time'])
dfTransaction['purchase_time'] = pd.to_datetime(dfTransaction['purchase_time'])
dfTransaction['time_diff']=(dfTransaction['purchase_time']-dfTransaction['signup_time']).astype('timedelta64[m]')
dfTransaction.head(10)

"""Le champ time_diff pourra représenter les deux champs signup_time et purchase_time dans notre modèle.

## **II-** Exploration et Statistiques descriptives :

> Explorer le jeu de données
"""

# Répartition des browsers dans le jeu de données
browser_count = dfTransaction['browser'].value_counts()
browser_count

# Repartition achat frauduleux et non frauduleux par browser 
nb_isnot_fraud = lambda is_fraud: is_fraud[is_fraud == 0].value_counts()
nb_is_fraud = lambda is_fraud: is_fraud[is_fraud == 1].value_counts()

functions_to_apply = {
    'is_fraud' : [nb_isnot_fraud, nb_is_fraud]
}

is_fraud_groupby = dfTransaction.groupby('browser').agg(functions_to_apply)

is_fraud_groupby.columns.set_levels(['nb_isnot_fraud', 'nb_is_fraud'], level=1, inplace = True)

is_fraud_groupby.head()

# Proportion (pourcentage) des fraudes par navigateur
dfTransaction.groupby('browser').agg(nombre=('is_fraud', 'size'), percentage_fraud=('is_fraud', 'mean'))

"""On constate qu'il y a légèrement plus de fraudes sur Chrome même si ce n'est pas très significatif par rapport aux autres navigateurs. La répartition est assez partagée."""

# Crosstab (répartition des fraudes/pas fraudes par browser)
table = pd.crosstab(dfTransaction.browser, dfTransaction.is_fraud)
table

# table de contingence (indépendance entre elles des variables browser et is_fraud)
from scipy.stats import chi2_contingency

resultats_test = chi2_contingency(table)
statistique = resultats_test[0]
p_valeur = resultats_test[1]
degre_liberte = resultats_test[2]

print(statistique, p_valeur, degre_liberte)

# p-value < 5% donc on rejette (H0 : "les deux variables browser et is_fraud sont indépendantes")

# V_Cramer (vérification browser / is_fraud)
import numpy as np 
def V_Cramer(table3, N):
    stat_chi2 = chi2_contingency(table)[0]
    k = table.shape[0]
    r = table.shape[1]
    phi = max(0,(stat_chi2/N)-((k-1)*(r-1)/(N-1)))
    k_corr = k - (np.square(k-1)/(N-1))
    r_corr = r - (np.square(r-1)/(N-1))
    return np.sqrt(phi/min(k_corr - 1,r_corr - 1))

V_Cramer(table, dfTransaction.shape[0])

#Le V_Cramer n'est pas très élevé.
#On en déduit qu'il n'y a pas une forte corrélation entre les deux variables browser et is_fraud.

# Application de ANNOVA (vérification age / is_fraud)
import statsmodels.api 

result = statsmodels.formula.api.ols('age ~ is_fraud', data=dfTransaction).fit()
table2 = statsmodels.api.stats.anova_lm(result)

table2

#la p-value (PR(>F)) est inférieur à 5% donc on peut dire que l'age influe sur is_fraud.

# voir si on a des identifiants (utilisateur, poste, addresse ip) qui se répètent
dfTransaction.nunique()

"""**On remarque que certains appareils ont fait plus de transactions que 
d'autres (leurs nombre est inférieur au nombre total des lignes du dataframe). Cela peut aussi être un indicateur de fraudes. Idem pour les addresses IP.**
"""

# proportion de la fraude entre homme et femme
dfTransaction.groupby('sex').agg(nombre=('is_fraud', 'size'), percentage_fraud =('is_fraud', 'mean'))

"""On constate que les fraudes sont légèrement plus d'origine masculine que féminine.

## **III-** Data processing :

Transformer les varibales catégorielles pour les préparer au Machine Learning qui suivra.
"""

# Transformation des varibales catégorielles
dfTransact = pd.get_dummies(dfTransaction, columns=['source','sex', 'browser'])
dfTransact.head(10)

"""## **IV-** Visualisation   

Visualiser certaines variables en fonction d'autres afin d'en comprendre aux mieux les corrélations et en détecter celles qui seraient les plus liées à des transactions frauduleuses.
"""

# Visualisation de time_diff en fonction de is_fraud
title = 'Distribution globale de la differnce entre l\'heure de la connexion\n et l\'heure de l\'achat'
sns.displot(data=dfTransaction, x=dfTransaction['time_diff'], hue=dfTransaction['is_fraud'])
plt.title(title)
plt.show()

"""**On peut constater que les transactions de très courtes durées (temps entre moment d'achat et moment de connexion) seront considérées comme frauduleuses.**  """

# Purchase_value en fonction de is_fraud
sns.displot(data=dfTransaction, x=dfTransaction['purchase_value'], hue=dfTransaction['is_fraud'], kind="kde", multiple="stack")
plt.title('Distribution globale de la variable montant d\'achat')
plt.show()

"""L'évolution de la valeur d'achat est similaire à celle des transactions qu'elles soient frauduleuses ou non."""

# Purchase_value en fonction de is_fraud
sns.displot(data=dfTransaction, x=dfTransaction['age'], hue=dfTransaction['is_fraud'], kind="kde", multiple="stack")
plt.title('Distribution globale de la variable age')
plt.show()

"""On constate la même chose pour l'âge."""

# Répartition de la fraude sur l'ensemble des données
fraud_val = dfTransaction.is_fraud.value_counts(sort = True, ascending = False)
labels = ('transactions normale','transaction frauduleuse')
fig1, ax1 = plt.subplots()
explode = (0.1, 0) 
ax1.pie(fraud_val.values,explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
plt.title('Proportion de la fraude sur la totalité des données')
plt.show()

"""Les transactions frauduleuses constituent à peu près 10% de l'ensemble des achats."""

# Barres sex, age, browser
fig, ax = plt.subplots(1,3,figsize=(19,5))
sns.barplot(x='sex', y='is_fraud', data=dfTransaction, ax=ax[0])

sns.barplot(x='source', y='is_fraud', data=dfTransaction, ax=ax[1])

sns.barplot(x='browser', y='is_fraud', data=dfTransaction, ax=ax[2])

"""On constate que la répartition de la fraude est similaire indépendamment du sexe, de la source ou du browser.

## **V-**  Machine learning    

Entraîner et évaluer les modèles de classification.

### Application des modèles de classification (Régression logistique) avant oversamling

> Préparer les données en séparant les variables explicatives de la variable cible.
>
> Séparer le jeu de données en deux : un jeu d'entraînement et un jeu de test.
>
> Instancier le(s) modèle(s).
>
> Entrainer le(s) modèle(s) : model.fit(X_train, y_train).
>
> Prédir : model.predict(X_test, y_test).
"""

# Drop pour ne garder que les variables explicatives à faire appliquer au modèle
dfTransactTemp = dfTransact.drop(['signup_time', 'purchase_time', 'device_id','user_id', 'ip_address'], axis = 1)
dfTransactTemp.head()

"""**Nous remarquons que les variables ne sont pas standadisées (age, purchase_value, time_diff par rapport au reste).** Trop d'écart entre les max et les min.   

**Nous allons donc standardiser les données à l'aide de MinMaxScaler**
"""

# define min max scaler
scaler = MinMaxScaler()

# transform data
dfTransactScaler = pd.DataFrame(scaler.fit_transform(dfTransactTemp.values), index=dfTransactTemp.index, columns=dfTransactTemp.columns)
dfTransactScaler.head()

"""On procède maintenant à la séparation des variables explicatives de la variable cible."""

# Instanciation du dataframe contenant les variables explicatives
X = dfTransactScaler.drop(['is_fraud'], axis = 1)

# Instanciation de la series contenant la variable cible 
y = dfTransactScaler['is_fraud']
X.head(10)

"""Séparer le jeu de données en deux (un jeu d'entraînement et un jeu de test) (train_test_split)."""

# On applique la fonction train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30, random_state=42)

# On affiche les dimensions des datasets après avoir appliquer la fonction 
print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

# On affiche les dimensions de y_train
print('\nles dimensions de y_train :')
y_train.value_counts()

"""Instancier le modèle, l'entrainer et prédir la target y_pred."""

# On instantie le modèle et on l'entraîne 
model_log=LogisticRegression(solver="newton-cg").fit(X_train,y_train)

# On prédit les y à partir de X_test
y_pred=model_log.predict(X_test)

# On affiche les coefficients obtenus
coeff=model_log.coef_

# On affiche la constante
intercept=model_log.intercept_

# On calcule les odd ratios
odd_ratios=np.exp(model_log.coef_)

# On crée un dataframe qui combine à la fois variables, coefficients et odd-ratios
resultats=pd.DataFrame(X.columns, columns=["Variables"])
resultats['Coefficients']=model_log.coef_.tolist()[0]
resultats['Odd_Ratios']=np.exp(model_log.coef_).tolist()[0]
resultats

"""Interprétation des odd-ratio significatives :  

1) **time_diff** : Lorsque la variable time_diff augmente de 3 fois alors cela diminue les chances d'appartenir à la classe positive d'environ 30 fois.
=> ce qui implique inversement que si time_diff diminue fortement cela fait augmenter les chances d'appartenir aux transactions frauduleuses. Ce qui confirme le graphique 'Distribution globale de la differnce entre l'heure de la connexion et l'heure de l'achat'.

2) **age** : Lorsque la variable age augmente de 19% alors cela augmente les chances d'appartenir à la classe positive d'environ 21%. Cette évolution sous-jacente de la fraude en fonction de l'age a aussi été constatée via le graphe 'Distribution globale de la variable age'.
"""

# On calcule la matrice de confusion et on l'affiche
print("\n Matrice de confusion :\n", confusion_matrix(y_test,y_pred))

"""La matrice de confusion est déséquilibrée vu qu'il n'y a ni de vrais positifs ni de faux positifs."""

# On affiche le recall_score du modèle 
print("recall_score :", recall_score(y_test, y_pred))
# un recall_score de 0 ce qui n'est pas interprétable vu notre jeu de données.

"""On remarque que y_train contient presque 10 fois plus de négatif que de postif. Ce déséquilibre de classes incite à effectuer un resampling (oversampling) pour ainsi rééquilibrer le jeu d'entrainement.

### Application du modèles de classification après oversamling

On a constaté un déséquilibre entre classes négatives et positives ce qui nous incite à appliquer un oversampling.

> Balancer le jeu de données (oversampling)
"""

# Oversampling : la solution pour résoudre le problème du déséquilibre des classes 
# consiste à rééchantillonner de manière aléatoire l'ensemble de données
# d'apprentissage
# Random Oversampling : duplique des exemples de la classe minoritaire dans l'ensemble de données d'apprentissage
oversample = SMOTE()
X_train, y_train = oversample.fit_resample(X_train, y_train)
y_train.value_counts()

# On réapplique le modèle et on l'entraîne 
model_log=LogisticRegression(solver="newton-cg").fit(X_train,y_train)

# On prédit les y à partir de X_test
y_pred=model_log.predict(X_test)

# On calcule la matrice de confusion et on l'affiche
print("\n Matrice de confusion :\n", confusion_matrix(y_test,y_pred))
tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()
print("\n Vrais négatifs:",tn,"\n Faux positifs:",fp,"\n Faux négatifs:",fn,"\n Vrais positifs:",tp)

"""**Ce qui nous intéresse ici ce sont les faux négatifs (les transactions frauduleuses qui passent pour non frauduleuses) et c'est ce qui va déterminer notre choix par rapport aux modèles que l'on appliquera.**"""

# Accuracy : On affiche l'accuracy du modèle 
print("Accuracy:",accuracy_score(y_test,y_pred))

"""> Nous obtenons une accuracy de 0.65, ce qui ne reflète pas de bonnes performances pour notre modèle.  
Regardons avec le recall_score : 
"""

print("recall_score :", recall_score(y_test, y_pred))
# Notre véritable mesure devrait être recall_score car nous pouvons nous permettre de classer 
# une transaction non frauduleuse comme frauduleuse, mais pas l'inverse.

"""
### Appliquer le modèle Arbre de décision

"""

### Essayons aussi l'arbre de décision pour avoir une autre perspective.
dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
y_pred2 = dtc.predict(X_test)

print('accuracy_score from DecisionTreeClassifier :', accuracy_score(y_test, y_pred2))

"""> Un accuracy score meilleur, mais quid du recall_score ?"""

print('recall_score from DecisionTreeClassifier :', recall_score(y_test, y_pred2))

"""Quid du f1_score ?"""

print('f1_score from DecisionTreeClassifier :' , f1_score(y_test, y_pred2))

"""> On constate un meilleur accuracy mais un recall_score légèrement plus bas.  

Matrice de confusion : 

"""

print('confusion_matrix from DecisionTreeClassifier:\n', confusion_matrix(y_test, y_pred2))

"""**On constate que les faux négatifs ont néanmoins augmenté. Les faux positifs ont quant à eux nettement diminué.**

### Appliquer le modèle Random Forest Classifier
"""

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

y_pred3 = rfc.predict(X_test)

print('accuracy_score from RandomForestClassifier :', accuracy_score(y_test, y_pred3))

print('recall_score from RandomForestClassifier :', recall_score(y_test, y_pred3))

print('f1_score from RandomForestClassifier :' , f1_score(y_test, y_pred3))

print('confusion_matrix from RandomForestClassifier: \n', confusion_matrix(y_test, y_pred3))

"""> **Nous choisirons finalement la régression logistique car les faux négatifs sont minimes dans ce cas.**

### Validation croisée   
Nous allons soumlettre notre modèle de régression logistique aux évaluations des performances et ce via une validation croisée :
"""

from sklearn.model_selection import cross_val_score

performances_ = cross_val_score(model_log, X, y, cv=5, scoring='accuracy')

from sklearn.model_selection import StratifiedKFold

folds_1 = StratifiedKFold(n_splits = 5, shuffle = False)

folds_2 = StratifiedKFold(n_splits = 5, shuffle = True, random_state=42)

performances_non_shuffled = cross_val_score(model_log, X, y, scoring='accuracy', cv=folds_1)

performances_shuffled = cross_val_score(model_log, X, y, scoring='accuracy', cv=folds_2)

print(" Les performances obtenues sur chacun des splits est de:\n\n", performances_, "\n\nEn moyenne cela donne donc une accuracy de:",np.mean(performances_),
      " \n\n Les performances obtenues sur chacun des splits avec un découpage successif des folds (Stratified-KFold) sans mélange est de:\n\n", performances_non_shuffled,"\n\nEn moyenne cela donne donc une accuracy de:",np.mean(performances_non_shuffled),
      "\n\n Les performances obtenues sur chaque des splits après un mélange des observations de la base est de:\n\n", performances_shuffled, "\n\nEn moyenne cela donne donc une accuracy de:",np.mean(performances_shuffled))

print("\nOn constate que les performances sont quasi équivalentes lorsque l'on mélange les observations et lorsqu'on ne les mélange pas.")

"""Interprétation graphique des performances :  

"""

# On définit la qualité de l'affichage

fig,ax = plt.subplots(dpi=120)

# On retire l'axe supérieur et l'axe de droite du graphique

ax.spines['right'].set_visible(False)

ax.spines['top'].set_visible(False)

# On conserve les graduations de l'axe du bas et de gauche

ax.yaxis.set_ticks_position('left') 

ax.xaxis.set_ticks_position('bottom') 

# On nomme les axes 

plt.xlabel("Splits")

plt.ylabel("Accuracy", rotation=0)

# On affiche les performances issues du dataset non mélangé

plt.plot(performances_non_shuffled,label="Non shuffled",color="#66d8bc")

# On affiche les performances issues du dataset  mélangé

plt.plot(performances_shuffled,label="Shuffled",color="#8f5ef4")

# On définit les graduations de l'axe des abscisses pour que ce soit des entiers 

plt.xticks(np.arange(0, 5, step=1))

# On affiche la légende des courbes 

plt.legend(loc=0)

# On affiche le titre du graphique

plt.title("\nPerformances obtenues par Stratified Cross Validation \n\n",fontsize=16);

# Le graphique permet de visualiser l'homogénéité parfaite des performances obtenues avant et après mélange.

"""## **VI**- Conclusion   

Au sein de ce projet nous avons utilisé plusieurs modèles :   
- un modèle de régression logistique LogisticRegression.
- un modèle d'arbres de décision DecisionTreeClassifier qui a donné une meilleure accuracy mais qui a le désavantage d'être moins interprétable.
- un modèle d'ensemble Learning RandomForestClassifier qui est tout comme le modèle précédent moins interprétable car il contient trop de faux négatifs par rapport aux deux autres.   

La problématique de déséquilibre de classes qu'on a rencontré en appliquant le modèle dans un premier temps nous a incité à appliquer un oversampling et de réentrainer le modèle pour avoir un résultat plus cohérent.

Le modèle LogisticRegression est le modèle de classification qui correspondrait le plus à notre problématique d'apprentissage supervisé car il prédit le moins de faux négatifs possibles.
"""

"""**Export du modèle**"""

from joblib import dump, load

dump(model_log, '../Model/LogisticRegression_model.joblib') 
dump(dtc, '../Model/DecisionTreeClassifier_model.joblib') 
dump(rfc, '../Model/RandomForestClassifier_model.joblib') 
